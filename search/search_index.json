{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Torch2Chip is an End-to-end Deep Neural Network compression toolkit designed for prototype accelerator designer for algorithm-hardware co-design with high-degree of algorithm customization.</p>"},{"location":"#news-update","title":"News &amp; Update","text":"<ul> <li>[04/15/2024]: Initial version of Torch2Chip is published together with the camera-ready version of our MLSys paper!</li> </ul>"},{"location":"#why-torch2chip","title":"Why Torch2Chip?","text":"<p>The current \"design-and-deploy\" workflow faces under-explored challenges in the current hardware-algorithm co-design community due to some unavoidable flaws:</p> <ul> <li> <p>Deep Learning framework: Although the state-of-the-art (SoTA) quantization algorithm can achieve ultra-low precision with negligible degradation of accuracy, the latest deep learning framework (e.g., PyTorch) can only support non-customizable 8-bit precision, data format (<code>torch.qint8</code>).</p> </li> <li> <p>Algorithms: Most of the current SoTA algorithm treats the quantized integer as an intermediate result, while the final output of the quantizer is the \"discretized\" floating-point values, ignoring the practical needs and adding additional workload to hardware designers for integer parameter extraction and layer fusion.</p> </li> <li> <p>Industry standard Toolkit: The compression toolkits designed by the industry (e.g., OpenVino) are constrained to their in-house product or a handful of algorithms. The limited degree of freedom in the current toolkit and the under-explored customization hinder the prototype ASIC-based accelerator.</p> </li> </ul> <p></p> <p>From the perspectives of the hardware designers, the conflicts from the DL framework, SoTA algorithm, and current toolkits formulate the cumbersome and iterative designation workflow of chip prototyping, which is what Torch2Chip aim to resolve.</p>"},{"location":"#what-is-torch2chip","title":"What is Torch2Chip?","text":"<p>Torch2Chip is a toolkit that enables customized model compression (e.g., quantization) with full-stack observability for customized hardware designers. Starting from the user-customized compression algorithms, Torch2Chip perfectly meet the bottom-level needs for the customized AI hardware designers: </p> <ul> <li> <p>[Model and Modules]: Unlike the open-sourced quantization algorithms, Torch2Chip does not require the user to have your customized moule or even model file (e.g., <code>resnet.py</code>). </p> </li> <li> <p>[Customize]: User just need to implement their own algorithm by following the proposed \"dual-path\" design. Torch2Chip will take care of the reamining step.</p> </li> <li> <p>[Decompose &amp; Extract]: Torch2Chip decompose the entire model down to the basic operations (e.g., <code>matmul</code>) , where the inputs are compressed by the user-customized algorithm.</p> </li> </ul> <p></p>"},{"location":"#authors","title":"Authors","text":"<p>Members of Seo Lab @ Cornell University led by Professor Jae-sun Seo.</p> <p>Jian Meng, Yuan Liao, Anupreetham, Ahmed Hasssan, Shixing Yu, Han-sok Suh, Xiaofeng Hu, and Jae-sun Seo.</p>"},{"location":"#cite-us","title":"Cite Us","text":"<p>Publication: Torch2Chip: An End-to-end Customizable Deep Neural Network Compression and Deployment Toolkit for Prototype Hardware Accelerator Design (Meng et al., MLSys, 2024).</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This work was supported in part by Samsung Electronics and the Center for the Co-Design of Cognitive Systems (CoCoSys) in JUMP 2.0, a Semiconductor Research Corporation (SRC) Program sponsored by the Defense Advanced Research Projects Agency (DARPA).</p>"},{"location":"basic%20modules/","title":"Basic Modules","text":"<p>The overall design of Torch2Chip follows the hierarchical design for customized compression algorithms, including both pruning and quantization. The following document introduces basic modules and the proposed \"Dual-Path\" design process.</p>"},{"location":"basic%20modules/#dual-path-design","title":"Dual Path Design","text":"<p>Different from the non-deployable computation path of most quantization algorithms, Torch2Chip employs the \"Dual-Path\" design, which separates the training and inference computation graph to support both floating-point-based \"fake quantization\" for training, and low precision-only (e.g., integer, low precision floating point) computation for hardware verification and parameter extraction.</p> <p>The Dual-Path design is the most important design rule of Torch2Chip, starting from the basic quantizer modules all the way to the layers and top-level trainers. </p> <p></p>"},{"location":"basic%20modules/#qbase","title":"<code>QBase</code>","text":"<p>Basic quantization module for customized quantization methods. [Source]</p> <pre><code>class _QBase(nn.Module):\n    r\"\"\"Base quantization method for weight and activation.\n\n    Args:\n    nbit (int): Data precision.\n    train_flag (bool): Training mode. \n\n    Methods:\n    trainFunc (input:Tensor): Training function of quantization-aware training (QAT)\n    evalFunc (input:Tensor): Forward pass function of inference. \n    inference(): Switch to inference mode. \n    \"\"\"\n    def __init__(self, nbit:int, train_flag:bool=True, unsigned:bool=True):\n</code></pre> <p>Parameters</p> <ul> <li><code>nbit</code>: Bit precision (e.g., 8, 6, 4, ...).</li> <li><code>train_flag</code>: Flag that signifies the training or inference path, which can be specified at initialization or on the model level <code>inference</code> mode.</li> <li><code>scale</code>: Scaling factor of quantization, (if the scaling factor is learnable (or computed based on the <code>nn.Parameter</code>), the scaling factor should be updated inside the quantization function <code>q</code>, and the buffer parameter <code>scale</code> will be utilized in the post-training conversion and export. </li> <li><code>zero_point</code>: Zero point (distribution offset) of the quantization process (<code>default = 0.0</code>). </li> </ul>"},{"location":"basic%20modules/#observer","title":"<code>observer</code>","text":"<p>Quantization observer (customizable for different methods) that monitors the dynamic range of the incoming floating point distribution (weight and activation). </p>"},{"location":"basic%20modules/#register_qparams","title":"<code>register_qparams</code>","text":"<p>Register the quantization parameters (<code>scale</code> and <code>zero point</code>) as buffer parameters.</p>"},{"location":"basic%20modules/#trainfunc","title":"<code>trainFunc</code>","text":"<p>Given the high precision floating point input (<code>torch.Tensor</code>), training path (<code>trainFunc</code>) performs the normal quantize &amp; dequantize (for integer) or quantize &amp; scaling (for low precision floating point) operation with the high precision floating-point values. </p>"},{"location":"basic%20modules/#evalfunc","title":"<code>evalFunc</code>","text":"<p>Evaluation path that performs the integer-only quantization (no dequantization or secaling). The output of the evaluation path only contains the quantized value, which will be directly fused in the T2C export (<code>nn2chip</code>) phase. </p> <p>Note: As the initial commit, the exact output of <code>evalFunc</code> is still in the data type of <code>torch.float32</code> or <code>torch.float16</code> (e.g., <code>torch.Tensor([8.])</code>), whereas the final output of the T2C will convert it into the actual integer data type. </p>"},{"location":"basic%20modules/#q","title":"<code>q</code>","text":"<p>Shared quantization operations between <code>trainFunc</code> and <code>evalFunc</code>: </p> <ul> <li>Compute / update the scaling factor <code>self.scale</code> and <code>self.zp</code></li> <li>Connect the quantization to the customized <code>Autograd</code> function (if necessary).</li> <li>Quantization / Dequantization steps activated by the flag of <code>self.train_flag</code>.</li> </ul>"},{"location":"basic%20modules/#qbaseconv2d","title":"<code>QBaseConv2d</code>","text":"<p>Basic convolutional layer compatible for T2C compression.  </p> <pre><code>class _QBaseConv2d(nn.Conv2d):\n    r\"\"\"\n    Basic low precision convolutional layer\n\n    Inherited from the base nn.Conv2d layer.\n\n    Args:\n    wbit (int): Weight quantization precision. \n    abit (int): Input quantization precision.\n    wq (_QBase): Weight quantizer. \n    aq (_QBase): Activation quantizer.   \n    \"\"\"\n    def __init__(self, in_channels:int, out_channels:int, kernel_size:int, stride:int=1, \n                padding:int=0, dilation:int=1, groups:int=1, bias:bool=True, wbit:int=32, abit:int=32, train_flag=True):\n        super(_QBaseConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n</code></pre> <p>Basic convolutional layer for low precision and sparse operations. [Source]</p> <p>Parameters:</p> <ul> <li>The configurations of the convolutional layer follows the settings of the vanilla <code>nn.Conv2d</code> of PyTorch. The detailed document is available on the official website (link)</li> <li><code>wbit (int)</code>: Target precision of <code>weight</code> (default = 32).</li> <li><code>abit (int)</code>: Target precision of activation (default = 32).</li> <li><code>train_flag (bool)</code>: Flag that signifies the training or inference path, which can be specified at initialization or on the model level <code>inference</code> mode.</li> <li><code>qweight (torch.Tensor)</code>: Buffer parameter which stores the low precision weights, which will be copied into the vanilla layer as the replica in the T2C export phase (<code>nn2chip</code>). To save memory during training, <code>qweight</code> will be registered only when the inference mode is activated. </li> </ul> <p>Attributes:</p> <ul> <li><code>wq (_QBase)</code>: Weight quantizer (default = <code>nn.Identity()</code>). </li> <li><code>aq (_QBase)</code>: Activation quantizer (default = <code>nn.Identity()</code>). </li> <li><code>yq (_QBase)</code>: Output quantizer (default = <code>nn.Identity()</code>). </li> </ul>"},{"location":"basic%20modules/#inference","title":"<code>inference</code>","text":"<p>Switch to inference mode: </p> <ul> <li>Switch the quantizers (<code>wq</code> and <code>aq</code>) to inference mode.</li> <li>Define the basic operation (<code>ops</code> = <code>ConvOPS</code>). </li> <li>Note: The basic conv operation (<code>ConvOPS</code>) only perform the low-precision only convolution. The bias will be fused externally into the subsequent fusers. </li> </ul>"},{"location":"basic%20modules/#qbaselinear","title":"<code>QBaseLinear</code>","text":"<p>Basic Linear layer compatible for T2C compression. </p> <pre><code>class _QBaseLinear(nn.Linear):\n    r\"\"\"Basic low precision linear layer\n\n    Inherited from the base nn.Linear layer.\n\n    Args:\n    wbit (int): Weight quantization precision. \n    abit (int): Input quantization precision.\n    wq (_QBase): Weight quantizer. \n    aq (_QBase): Activation quantizer.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = True, wbit:int=32, abit:int=32, train_flag=True):\n        super(_QBaseLinear, self).__init__(in_features, out_features, bias)\n</code></pre> <p>Basic fully-connected layer for low precision and sparse operations</p> <p>Parameters:</p> <ul> <li>The configurations of the convolutional layer follows the settings of the vanilla <code>nn.Linear</code> of PyTorch. The detailed document is available on the official website (link)</li> <li><code>wbit (int)</code>: Target precision of <code>weight</code> (default = 32).</li> <li><code>abit (int)</code>: Target precision of activation (default = 32).</li> <li><code>train_flag (bool)</code>: Flag that signifies the training or inference path, which can be specified at initialization or on the model level <code>inference</code> mode.</li> <li><code>qweight (torch.Tensor)</code>: Buffer parameter which stores the low precision weights, which will be copied into the vanilla layer as the replica in the T2C export phase (<code>nn2chip</code>). To save memory during training, <code>qweight</code> will be registered only when the inference mode is activated. </li> </ul> <p>Attributes:</p> <ul> <li><code>wq (_QBase)</code>: Weight quantizer (default = <code>nn.Identity()</code>). </li> <li><code>aq (_QBase)</code>: Activation quantizer (default = <code>nn.Identity()</code>). </li> <li><code>yq (_QBase)</code>: Output quantizer (default = <code>nn.Identity()</code>). </li> </ul>"},{"location":"basic%20modules/#inference_1","title":"<code>inference</code>","text":"<p>Switch to inference mode: </p> <ul> <li>Switch the quantizers (<code>wq</code> and <code>aq</code>) to inference mode.</li> <li>Define the basic operation (<code>ops</code> = <code>MatMul</code>). </li> <li>Note: The basic conv operation (<code>MatMul</code>) only perform the low-precision only Matrix Multiplication. The bias will be fused externally into the subsequent fusers. </li> </ul>"},{"location":"basic%20modules/#qattention","title":"<code>QAttention</code>","text":"<p>Basic Attention Module compatible for T2C compression. The original implementation is adopted from <code>timm</code>. </p> <pre><code>class QAttention(nn.Module):\n    def __init__(\n            self,\n            dim:int,\n            num_heads, \n            qkv_bias=False,\n            qk_norm=False, \n            attn_drop=0.0,\n            proj_drop=0.0,\n            norm_layer=nn.LayerNorm\n    ):\n        super().__init__()\n        assert dim % num_heads == 0,\"dim should be divisible by num_heads\"\n</code></pre> <p><code>QAttention</code> module can be considered as a direct replica of the vanilla <code>Attention</code> module of the <code>timm</code> library. The input arguments shares the same layout as the vanilla Attention. The major difference is the separation between the training and inference paths. </p> <p>Attributes:</p> <ul> <li><code>xq (_QBase)</code>: Input quantizer. </li> <li><code>qproj (_QBase)</code>: Quantize the MatMul results between Attention and Value (<code>self.attnv</code>)</li> <li><code>qqkv (_QBase)</code>: Shared quantizer of Q, K, and V.</li> </ul> <p>Training path:</p> <p>Normal attention-softmax-projection operations as before</p> <pre><code>def trainFunc(self, q, k, v):\n    attn = q @ k.transpose(-2, -1)  # out dim = token x token\n    attn = self.attn_scale(attn)\n\n    attn = F.softmax(attn, dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = attn @ v                    # out dim = token x head_dim\n    x = self.qproj(x)\n\n    return x\n</code></pre> <p>Inference path:</p> <pre><code>def evalFunc(self, q, k, v):\n    attn = self.qk(q, k.transpose(-2, -1))\n    attn = self.attn_scale(attn)\n\n    attn = F.softmax(attn, dim=-1)\n    attn = attn.mul(255.).round()\n\n    attn = self.attn_drop(attn)\n\n    x = self.attnv(attn, v)\n    x = self.qproj(x)\n\n    return x.float()\n</code></pre> <p>The matrix multiplication operands are replaced as the <code>IntMatMul</code> modules. During the export phase of T2C, the intermediate results of the matmul will be exported based on the location of the <code>IntMatMul</code>:</p> <ul> <li><code>qk</code>: Matrix multiplication operand between Q and K.</li> <li><code>attnv</code>: Matrix multiplication operand between attention and V.</li> </ul> <p>By default, the output of the softmax is quantized to unsigned INT8 (0 to 255) in the inference path. </p> <ul> <li>The potential accuracy degradation caused by the low precision softmax will be resolved in future work. </li> </ul>"},{"location":"basic%20modules/#inference_2","title":"<code>inference</code>","text":"<p>Switch to inference mode: </p> <ul> <li>Switch all the weight quantizers (<code>self.qkv</code> and <code>self.proj</code>) to inference mode. </li> <li>Switch all the tensor quantizer (<code>self.xq</code>, <code>self.qqkv</code> <code>self.qproj</code>) to inference mode.</li> <li>Activate the basic <code>ops</code> and replace the normal operation (<code>torch.nn.functional.linear</code>) to <code>Matmul</code>. </li> <li><code>self.qk(MatMul)</code>: Matrix multiplication operator between Query (Q) and Key (K).</li> <li><code>self.attv(MatMul)</code>: Matrix multiplication operator between Attention and Value (V). </li> </ul>"},{"location":"basic%20modules/#qwindowattention","title":"<code>QWindowAttention</code>","text":"<p>Basic Attention Module for SwinTransformer compatible for T2C compression. The original implementation is adopted from <code>timm</code>. </p> <pre><code>class QWindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports shifted and non-shifted windows.\n\n    Full precision version is adopted from timm. \n    Quantizers are added for T2C\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            head_dim = None,\n            window_size = 7,\n            qkv_bias: bool = True,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            num_heads: Number of attention heads.\n            head_dim: Number of channels per head (dim // num_heads if not set)\n            window_size: The height and width of the window.\n            qkv_bias:  If True, add a learnable bias to query, key, value.\n            attn_drop: Dropout ratio of attention weight.\n            proj_drop: Dropout ratio of output.\n        \"\"\"\n</code></pre> <p>Similar to the <code>QAttention</code> module, <code>QWindowAttention</code> is a dedicated module designed for low precision attention and T2C conversion computation. Similarly, <code>QWindowAttention</code> follows the similar design logic with <code>training</code> and <code>inference</code> paths.</p> <p>Attributes:</p> <ul> <li><code>xq (_QBase)</code>: Input quantizer. </li> <li><code>qproj (_QBase)</code>: Quantize the MatMul results between Attention and Value (<code>self.attnv</code>)</li> <li><code>qqkv (_QBase)</code>: Shared quantizer of Q, K, and V.</li> </ul> <p>Training path:</p> <p>Normal attention-softmax-projection operations with mask + High precision softmax:</p> <pre><code>def trainFunc(self, q, k, v, B:int, N:int, mask: torch.Tensor = None):\n        attn = q @ k.transpose(-2, -1)\n        attn = self.attn_scale(attn)\n        attn = attn + self._get_rel_pos_bias()\n\n        if mask is not None:\n            num_win = mask.shape[0]\n            attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n        x = x.transpose(1, 2).reshape(B, N, -1)\n        x = self.qproj(x)\n        return x\n</code></pre> <p>Inference path:</p> <pre><code>def evalFunc(self, q, k, v, B:int, N:int, mask: torch.Tensor = None):\n        q, k, v = q.double(), k.double(), v.double()\n        attn = self.qk(q, k.transpose(-2, -1))\n\n        # pos_bias is fused into the scaler\n        attn = self.attn_scale(attn)\n        attn = attn + self._get_rel_pos_bias()\n\n        if mask is not None:\n            num_win = mask.shape[0]\n            attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n\n        attn = self.softmax(attn)\n        attn = attn.mul(255.).round()\n\n        attn = self.attn_drop(attn)\n\n        x = self.attnv(attn, v)\n        x = x.transpose(1, 2).reshape(B, N, -1)\n        x = self.qproj(x)\n        return x.float()\n</code></pre>"},{"location":"basic%20modules/#inference_3","title":"<code>inference</code>","text":"<p>Switch to inference mode: </p> <ul> <li>Switch all the weight quantizers (<code>self.qkv</code> and <code>self.proj</code>) to inference mode. </li> <li>Switch all the tensor quantizer (<code>self.xq</code>, <code>self.qqkv</code> <code>self.qproj</code>) to inference mode.</li> <li>Activate the basic <code>ops</code> and replace the normal operation (<code>torch.nn.functional.linear</code>) to <code>Matmul</code>. </li> <li><code>self.qk(MatMul)</code>: Matrix multiplication operator between Query (Q) and Key (K).</li> <li><code>self.attv(MatMul)</code>: Matrix multiplication operator between Attention and Value (V). </li> </ul>"},{"location":"basic%20modules/#qbertselfattention-beta","title":"<code>QBertSelfAttention</code> (Beta)","text":"<p>Basic Attention Module from BERT (<code>transformer</code> developed by HuggingFace) compatible for T2C compression. </p> <pre><code>class QBertSelfAttention(nn.Module):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n</code></pre> <p>Currently, Torch2Chip mainly support the encoding process based on the compressed BERT model. </p> <ul> <li>[ ] [04/18/24]: We will support the decoder-based language model in the near future. </li> </ul> <p>The input <code>config</code> and <code>position_embedding_type</code> inherits directly from the <code>BertSelfAttention</code> module of <code>transformer==4.39.2</code> (link). </p> <p>Attributes:</p> <ul> <li><code>xq (_QBase)</code>: Input quantizer. </li> <li><code>qquery (_QBase)</code>: Dedicated quantizer of Query.</li> <li><code>qkey (_QBase)</code>: Dedicated quantizer of Key.</li> <li><code>qvalue (_QBase)</code>: Dedicated quantizer of Value.</li> <li><code>qkv_deq (_MulShift)</code>: Dequantizer affter Attention and Value Matmul.   </li> </ul> <p>Unlike the <code>Attention</code> module and the <code>WindowAttention</code> module that we used for Vision Transformers, the self-attention of BERT model separates the computation of Query, Key, and Value layers with 3  dedicated layers.</p> <p>Training Path:</p> <p>The <code>trainFunc</code> of the module inherits the same forward pass as the vanilla module of <code>transformer</code> library. The quantizers (with training mode) are plugged into the <code>trainFunc</code>. For instance, for the fake-quantization of Query, we have:</p> <pre><code>hidden_states = self.xq(hidden_states)\nmixed_query_layer = self.query(hidden_states)\n\n# low precision Q\nmixed_query_layer = self.qquery(mixed_query_layer)\n</code></pre> <p>Evaluation Path:</p> <pre><code>def evalFunc(self, hidden_states: torch.Tensor, **kwargs) -&gt; Tuple[torch.Tensor]:\n        hidden_states = self.xq(hidden_states)\n\n        # Q\n        mixed_query_layer = self.query(hidden_states)\n        mixed_query_layer = self.qquery(mixed_query_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        # K\n        key = self.key(hidden_states)\n        key_layer = self.qkey(key)\n        key_layer = self.transpose_for_scores(key_layer)\n\n        # V\n        value = self.value(hidden_states)\n        value_layer = self.qvalue(value)\n        value_layer = self.transpose_for_scores(value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = self.qk(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = self.attn_scale(attention_scores)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        # round the attention score to 8-bit (fixed)\n        attention_probs = attention_probs.mul(256.).round()\n\n        context_layer = self.attnv(attention_probs, value_layer)\n        context_layer = self.qkv_deq(context_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n        return outputs\n</code></pre>"},{"location":"compression/","title":"DNN Compression","text":"<p>Driven by the desire of energy-efficient DNN computation, compression aims to either reduce the precision of the model representation (quantization) or remove the redundant paramters from the model itself (pruning). In Torch2Chip, we support the collective compression with both low precision quantization and sparsity. </p>"},{"location":"compression/#quantization","title":"Quantization","text":"<p>Quantization has been one of the most dominant methods for model compression. Essentially, quantization can be considered as a process of aligning the original floating-point distribution with a pre-defined \"grid\" with different data precisions (e.g., INTX, FPX). </p>"},{"location":"compression/#low-precision-integer-quantization","title":"Low Precision Integer Quantization","text":"<p>The integer-only quantization can be generalized as:</p> \\[ X_Q = \\text{round}(\\frac{X_{FP}}{S_x}) + \\text{zero point} \\] <p>Where \\(S_x\\) represents the scaling factor of the quantization. </p> <p>In practice, the quantization process is implemented as:</p> <pre><code>xq = torch.round(x / self.scale)\n</code></pre> <p>Where <code>scale</code> is defined by the range of the floating point distribution (clipped or non-clipped) and the target data precision <code>n</code>. </p> <p></p>"},{"location":"compression/#scaling-factor-size-of-the-interval","title":"Scaling factor: Size of the interval","text":"<p>Scaling factor defines the interval between two adjacent levels after dequantization. Naturally, given the distribution with the floating point range = \\([\\alpha, \\beta]\\) and precision \\(n\\)\u200b, the scaling factor is defined as:</p> \\[ S = \\frac{\\beta - \\alpha}{2^n - 1} \\] <p>Naturally, it is easy to tell that the total number of \"intervals\" after quantization is \\(2^n-1\\), defined by the lower bound and upper bound of the quantization range (not floating point range), which is also characterized by the \"signed\" and \"unsigned\" data format. </p> <ul> <li>Signed integer: Upper bound <code>qub</code> = \\(2^{n-1}-1\\) ; Lower bound <code>qlb</code> = \\(-2^{n-1}\\)</li> <li>Unsigned integer: Upper bound <code>qub</code> = \\(2^n-1\\); Lower bound <code>qlb</code> = 0.</li> </ul> <p>Both signed and unsigned integer leads to the total number of intervals = \\(2^n-1\\). </p>"},{"location":"compression/#zero-point-distribution-shifter","title":"Zero Point: Distribution Shifter","text":"<p>In practice, the data range of the floating point distribution could be mismatched with the quantization range of the target data format (signed or unsigned). As a result, an offset is needed to correct the range of distribution. </p> <p>Specifically, zero point is computed as the difference between the rounded floating point range and target lower bound of quantization: </p> <pre><code>zero_point = self.qlb - torch.round(self.lb / self.scale)\n</code></pre> <p>Where <code>self.lb</code> , <code>self.qlb</code> represents the low bound of the original distribution (high precision) and the lower bound of quantization range. </p> <p>In Torch2Chip, scaling factors and zero point are defined as the basic \"quantization parameters\" (<code>qparams</code>), which will be registered directly under the <code>_QBase</code> : </p> <pre><code>def register_qparams(self):\n    # register quantization scaler and zero point\n    self.register_buffer(\"scale\", torch.tensor(1.0))\n    self.register_buffer(\"zero_point\", torch.tensor(0.0))\n</code></pre> <p>Overall, the quantization process is summarized as:</p> <pre><code>xr = torch.round(x / self.scale) + self.zero_point\nxq = torch.clamp(xr, min=self.qlb, max=self.qub)\nxq = xq.sub(self.zero_point)\n\n# dequantize\nif self.dequantize:\n    xdq = xdq.mul(self.scale)\n</code></pre>"},{"location":"compression/#observers","title":"Observers","text":"<p>In Torch2Chip, the statistics (e.g., lower bound, upper bound) of the incoming tensor is captured by different observers, corresponding to different quantization method proposed by the prior research works.  Starting from the base quantizer (<code>_QBase</code>), the observer is a necessary part of the quantization process.</p>"},{"location":"compression/#baseobserver-source","title":"<code>BaseObserver</code> [Source]","text":"<pre><code>class BaseObserver(nn.Module):\n    def __init__(self, nbit:int, unsigned:bool=True):\n        super().__init__()\n</code></pre> <p>Attributes:</p> <ul> <li><code>nbit</code>: Target precision of quantization.</li> <li><code>unsigned</code>: Target data type (signed or unsigned integer).</li> <li><code>initialize</code>: Initialization flag. </li> </ul>"},{"location":"compression/#register_rangeself","title":"<code>register_range(self)</code>","text":"<p>Register the buffer to record the numerical range in the high-precision floating point domain. </p> <ul> <li><code>self.lb</code>: Lower bound of the floating point distribution (default = <code>torch.tensor(\"-inf\")</code>)</li> <li><code>self.ub</code>: Upper bound of the floating point distribution (default = <code>torch.tensor(\"inf\")</code>). </li> </ul>"},{"location":"compression/#calculate_qparamself-xtorchtensor","title":"<code>calculate_qparam(self, x:torch.Tensor)</code>","text":"<p>Calculate the quantization parameters (scaling factor and zero point) based on the updated upper and lower bound. </p> <p>Output:</p> <p>Calculated quantization parameter </p>"},{"location":"compression/#basechannelwiseobserver-source","title":"<code>BaseChannelWiseObserver</code> [Source]","text":"<p>Inherited from <code>BaseObserver</code>,  the channel-wise observer capture the numerical range along the channels of weight tensor. </p> <ul> <li> <p><code>num_channels</code>: Number of channels of a given layer, defined at the begining of compression. </p> </li> <li> <p><code>self.lb</code>: Lower bound of the floating point tensor (shape = <code>self.num_channels</code>). </p> </li> <li><code>self.ub</code>: Upper bound of the floating point tensor (shape = <code>self.num_channels</code>). </li> </ul>"},{"location":"compression/#basetokenwiseobserver-source","title":"<code>BaseTokenWiseObserver</code> [Source]","text":"<p>Inherited from <code>BaseObserver</code>, the token-wise observer capture the numerical range along the token dimension of the weight tensor. </p> <ul> <li> <p><code>num_tokens</code>: Number of tokens of a given model, defined at the begining of compression. </p> </li> <li> <p><code>self.lb</code>: Lower bound of the floating point tensor (shape = <code>self.num_tokens</code>).  </p> </li> <li><code>self.ub</code>: Upper bound of the floating point tensor (shape = <code>self.num_tokens</code>). </li> </ul>"},{"location":"compression/#convert-faked-quantized-ops-to-low-precision-ops","title":"Convert Faked Quantized Ops to Low-precision Ops","text":"<p>By default, the quantized tensor \\(X_Q\\) requires dequantization during training (PTQ, QAT) to ensure the numerical stability: $$ X_{DQ} = S_X(X_Q - \\text{zero point}) $$ Given the weight tensor \\(X^L\\) and weight tensor \\(W^L\\) of layer \\(L\\), the dequantize process can be factorized out: $$ Y = S_WS_X(X_Q \\cdot W_Q) $$ For simplicity, we assume the zero point = 0. By doing so, the operation (e.g., MatMul, Convolution) between \\(X_Q, W_Q\\) is compressed. </p> <p>In practice, the accuracy-driven quantization requires different granularity schemes for weight and activation. Therefore, the fusion of the scaling factors should be individually treated for different schemes (ViT Example).</p>"},{"location":"compression/#quantization-methods","title":"Quantization Methods","text":"<p>The objective of Torch2Chip is enabling the systematic design of compression. Starting from customized compression algorithm, all the way to the fully quantized and observable tensors (intermediate results).</p> <p>In terms of customization, Torch2Chip support recent state-of-the-art (SoTA) quantization algorithms, together with different quantization granularities (tensor-wise, token-wise, channel-wise).</p> <ul> <li>Note: We are continously adding different quantization algorithms into Torch2Chip! </li> </ul> <p>In Torch2Chip, all the quantization methods follows the structure of 1) Quantizer (inherited from <code>_QBase</code>),  2) Observer (<code>_BaseObserver</code>), and 3) Properly calculated and stored quantization parameters (<code>qparams</code>).</p>"},{"location":"compression/#minmax-quantization","title":"MinMax Quantization","text":"<p>Simplest quantization strategy which takes the minimal and maximal value of the distribution as the upper bound of lower bound of quantization.</p>"},{"location":"compression/#minmaxquantizer-source","title":"<code>MinMaxQuantizer</code> [Source]","text":"<pre><code>class MinMaxQuantizer(_QBase):\n  def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = True):\n        super().__init__(nbit, train_flag, unsigned)\n\n            # observer\n        self.observer = MinMaxObserver(nbit=self.nbit, unsigned=self.unsigned)\n</code></pre> <p>Tensor-wise quantizer with single scaling factor and zero point, which are computed by observing the minimal and maximal bound of the incoming distribution (e.g., weight. activation). </p>"},{"location":"compression/#minmaxchannelwiseweightquantizer-source","title":"<code>MinMaxChannelWiseWeightQuantizer</code> [Source]","text":"<pre><code>class MinMaxChannelWiseWeightQuantizer(MinMaxQuantizer):\n    def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = False, num_channels:int = 1):\n        self.num_channels = num_channels\n        super().__init__(nbit, train_flag, unsigned)\n\n        # observer\n        self.observer = MinMaxChannelWiseWeightObserver(nbit=self.nbit, unsigned=unsigned)\n\n        # qparams\n        self.register_qparams()\n</code></pre> <p>Channel-wise quantizer for weight tensors. The observer (<code>MinMaxChannelWiseWeightObserver</code>) and quantization parameters (<code>scale</code> and <code>zero_point</code>) are designed to match the channel-dimension of the incoming tensor, for both convolutional neural network and transformer architectures. </p>"},{"location":"compression/#minmaxtokenwisequantizer-source","title":"<code>MinMaxTokenWiseQuantizer</code> [Source]","text":"<pre><code>class MinMaxTokenWiseQuantizer(MinMaxQuantizer):\n    def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = True, num_tokens: int = 197):\n        self.num_tokens = num_tokens\n        super().__init__(nbit, train_flag, unsigned)\n\n        # observer\n        self.observer = MinMaxTokenWiseObserver(nbit=self.nbit, unsigned=self.unsigned, num_tokens=num_tokens)\n\n        # qparams\n        self.register_qparams()\n</code></pre> <p>Token-wise quantizer for activation tensors.  The observer (<code>MinMaxChannelMinMaxTokenWiseObserver</code>) and quantization parameters (<code>scale</code> and <code>zero_point</code>) are designed to match the token-dimension of the incoming tensor, for transformer architecture only.</p>"},{"location":"compression/#minmaxobserver-source","title":"<code>MinMaxObserver</code> [Source]","text":"<pre><code>class MinMaxObserver(BaseObserver):\n    def __init__(self, nbit: int, unsigned: bool = True):\n        super().__init__(nbit, unsigned)\n</code></pre> <p>Tensor-wise Observer. Track the minimal and maximal value of the incoming tensor (for weight or activation) in the forward pass. Given the total number of calibration samples = \\(N\\), <code>MinMaxObserver</code> continously update the upper bound and lower bound of the quantization range (<code>lb</code> and <code>ub</code>). </p> <pre><code>lb = torch.min(self.lb, min_val)\nub = torch.max(self.ub, max_val)\n\n# update bound\nself.lb.copy_(lb)\nself.ub.copy_(ub)\n</code></pre> <p>Note: The quantization parameters of <code>MinMaxQuantizer</code> and <code>MinMaxObserver</code> will remain fixed during the post-quantization inference. Dynamically calculating the quantization parameter on the fly leads to the repetitive sorting and hardware overhead. </p>"},{"location":"compression/#minmaxchannelwiseweightobserver-source","title":"<code>MinMaxChannelWiseWeightObserver</code> [Source]","text":"<pre><code>class MinMaxChannelWiseWeightObserver(BaseChannelWiseObserver):\n    def __init__(self, nbit: int, unsigned: bool = True, num_channels:int=1):\n        super().__init__(nbit, unsigned, num_channels)\n</code></pre> <p>Channel-wise observer for weight tensors. Generating the channel-wise quantization boundaries (<code>lb</code> and <code>ub</code>), observing the channel-wise minimal and maximal value and further calculate the channel-wise scaling factor (<code>scale</code>) and zero point (<code>zero_point</code>). </p> <p>Attributes / Methods:</p> <ul> <li><code>num_channels</code>: Number of channels of the weight tensor. </li> <li><code>reshape(self)</code>: Reshape the incoming tensor to isolate the channel dimension (for CNN or transformers).</li> </ul>"},{"location":"compression/#minmaxtokenwiseobserver-source","title":"<code>MinMaxTokenWiseObserver</code> [Source]","text":"<p>Token-wise observer for activation tensors. Generating the channel-wise quantization boundaries (<code>lb</code> and <code>ub</code>), observing the channel-wise minimal and maximal value and further calculate the token-wise scaling factor (<code>scale</code>) and zero point (<code>zero_point</code>). </p>"},{"location":"compression/#learned-step-size-quantization-lsq-reference","title":"Learned Step Size Quantization (LSQ) [Reference]","text":"<p>Learned step size quantizer considers the scaling factor of quantization as a learnable parameter (<code>torch.nn.Parameter</code>). </p>"},{"location":"compression/#lsq-source","title":"<code>LSQ</code> [Source]","text":"<pre><code>class LSQ(_QBase):\n    def __init__(self, nbit: int = 8, train_flag: bool = True, unsigned: bool = True):\n        super().__init__(nbit, train_flag, unsigned)\n        self.train_flag = train_flag\n        self.unsigned = unsigned\n\n        # initialization flag\n        self.initialize = False\n\n        # observer\n        self.observer = LSQObserver(nbit=self.nbit, unsigned=self.unsigned)\n\n        # register q parameters\n        self.register_qparams()\n</code></pre> <p>Tensor-wise quantizer that optimizes the learnable scaling factor (<code>delta</code>) throughout the layer-wise calibration process. </p> <p>Attributes:</p> <ul> <li><code>register_qparams(self)</code> : Together with the quantization parameters, the learnable scaling factor <code>delta</code> is registered as an trainable <code>torch.nn.Parameter</code>:</li> </ul> <pre><code>self.register_parameter(\"delta\", torch.nn.Parameter(torch.tensor(1.0)))\n</code></pre> <ul> <li><code>initialize</code>: Flag controls the initialization of the learnable scaling factor (default = False). </li> </ul> <p>Note: LSQ is designed for low precision quantization for the activation tensors. </p>"},{"location":"compression/#lsqtokenwise-source","title":"<code>LSQTokenWise</code> [Source]","text":"<pre><code>class LSQTokenWise(LSQ):\n    def __init__(self, nbit: int = 8, train_flag: bool = True, unsigned: bool = True, num_tokens: int = 197):\n        self.num_tokens = num_tokens\n        super().__init__(nbit, train_flag, unsigned)\n\n        self.observer = LSQTokenWiseObserver(nbit=self.nbit, unsigned=self.unsigned)\n\n        # register q parameters\n        self.register_qparams()\n</code></pre> <p>Token-wise quantizer for activation tensors. The learnable step size (<code>delta</code>) is initialized as a 1-dimensional tensor with the size = total number of tokens for each inference. Given the requirement of the fixed token size, <code>LSQTokenWise</code> quantizer is suitable for vision transformer or single pass LLM inference with fixed sequence length, e.g., 197 or 2048, respectively. </p>"},{"location":"compression/#lsqobserver-source","title":"<code>LSQObserver</code> [Source]","text":"<pre><code>class LSQObserver(BaseObserver):\n    def __init__(self, nbit: int, unsigned: bool = True):\n        super().__init__(nbit, unsigned)\n</code></pre> <p>Tensor-wise observer for activation tensors. <code>LSQObserver</code> allocates the best initial values before the PTQ training starts. </p> <ul> <li><code>quantize</code>: One shot quantization with the given candidate scaling factor and zero point. </li> <li><code>calculate_qparam</code>: Find the optimal value of scaling factor by gradually reduce the candidate scaling factor:</li> </ul> <pre><code>for i in range(100):\n    new_min = self.lb * (1.0 - (i * 0.01))\n    new_max = self.ub * (1.0 - (i * 0.01))\n</code></pre>"},{"location":"compression/#lsqtokenwiseobserver-source","title":"<code>LSQTokenWiseObserver</code> [Source]","text":"<pre><code>class LSQTokenWiseObserver(BaseTokenWiseObserver):\n    def __init__(self, nbit: int, unsigned: bool = True, num_tokens: int = 197):\n        super().__init__(nbit, unsigned, num_tokens)\n</code></pre> <p>Token-wise observer which initialize the token-wise scaling factor with the best candidate and lowest quantization error. </p>"},{"location":"compression/#adaptive-rounding-reference","title":"Adaptive Rounding [Reference]","text":"<p>Weight quantization method with learnable and adaptive rouding intervals. </p>"},{"location":"compression/#adaround-source","title":"<code>AdaRound</code> [Source]","text":"<pre><code>class AdaRound(_QBase):\n    def __init__(self, nbit: int, train_flag: bool = True, weights: torch.Tensor=None, unsigned=False):\n        super().__init__(nbit, train_flag, unsigned)\n        self.iter = 0\n\n        # initialize the alpha\n        self.init_flag = True\n\n        # parameters\n        self.gamma, self.zeta = -0.1, 1.1\n        self.beta = 2/3\n\n        # define the observer\n        self.observer = AdaRoundObserver(nbit=self.nbit, unsigned=self.unsigned)\n\n        # register the learnable parameters\n        self.register_alpha(weights)\n</code></pre> <p>Tensor-wise weight quantizer with element-wise trainable interval (<code>alpha</code>), which determines the rounding direction of each single weight element. </p> <p>Attributes and Methods:</p> <ul> <li><code>register_alpha</code>: Register the element-wise rounding offset. </li> </ul> <p>To ensure the differentiability, the training path <code>trainFunc</code> performs the soft rounding on the weight tensor. While in the inference path <code>evalFunc</code>, the offset has been rounded to either 0 or 1, indicating the direction of \"round up\" or \"stay at current value\".</p> <pre><code>soft_shift = self.h()\n\nif self.train_flag or self.training:\n   xada = xfloor + soft_shift\nelse:\n   xada = xfloor + self.alpha.ge(0.0).float()\n\nxq = xada + self.zero_point\n\n# integer representation\noutput = torch.clamp(xq, self.observer.qlb, self.observer.qub).sub(self.zero_point)\n</code></pre>"},{"location":"compression/#qdrop-reference","title":"QDrop [Reference]","text":"<p>Randomly disable the quantization and partially activate the full-precision operation during post-training quantization calibration, designed for activation quantization.</p> <p>QDrop is inherited from LSQ, while introducing a drop out probability (default = 0.5). </p>"},{"location":"compression/#qdrop-source","title":"<code>QDrop</code> [Source]","text":"<pre><code>class QDrop(LSQ):\n    def __init__(self, nbit: int = 8, train_flag: bool = True, unsigned: bool = True, drop_prob:float=0.5):\n        super().__init__(nbit, train_flag, unsigned)\n        self.drop_prob = drop_prob\n</code></pre> <p>Attributes:</p> <ul> <li><code>drop_prob</code>: Drop out probability, randomly replace (mask) the quantized tensor by the original full precision elements. </li> </ul>"},{"location":"compression/#qdroptokenwise-source","title":"<code>QDropTokenWise</code> [Source]","text":"<p>Token-wise quantizer based on QDrop algorithm, inherited from <code>LSQTokenWise</code>. </p> <pre><code>class QDropTokenWise(LSQTokenWise):\n    def __init__(self, nbit: int = 8, train_flag: bool = True, unsigned: bool = True, drop_prob:float=0.5):\n        super().__init__(nbit, train_flag, unsigned)\n        self.drop_prob=0.5\n</code></pre> <p>Attributes:</p> <ul> <li><code>drop_prob</code>: Drop out probability, randomly replace (mask) the quantized tensor by the original full precision elements. </li> </ul>"},{"location":"compression/#smoothquant-reference","title":"SmoothQuant [Reference]","text":"<p>Collective quantizer for both weight and activation, designed for transformer architecture. </p>"},{"location":"compression/#smoothquantizer-source","title":"<code>SmoothQuantizer</code> [Source]","text":"<pre><code>class SmoothQuantizer(_QBase):\n    def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = True):\n        super().__init__(nbit, train_flag, unsigned)\n\n        # smoother\n        self.smoother = MulShift()\n\n        # observer\n        self.observer = MinMaxObserver(nbit=self.nbit, unsigned=self.unsigned)\n</code></pre> <p>Tensor-wise quantizer for both activation or weights. Smooth out the distribution to alleviate the impact of long-tailed distribution. The quantization scheme is inherited from the <code>MinMaxQuantizer</code> with the tensor-wise <code>MinMaxObserver</code>. </p> <p>Attributes:</p> <ul> <li><code>smoother</code>: Distribution smoother (<code>MulShift</code>), the smooth factor should be assigned properly before the PTQ starts, as shown in <code>SmoothQuantPTQViT</code> (source). </li> <li><code>observer</code>: Tensor-wise observer <code>MinMaxObserver</code>. </li> </ul> <p>The input tensor (<code>torch.Tensor</code>) will be smoothed out before quantization:</p> <pre><code>def q(self, x:torch.Tensor):\n  # smooth out the distribution\n  x = self.smoother(x)\n\n  if self.train_flag:\n    # go through the observer\n    delta, zero_point = self.observer(x)\n\n    self.scale.data = 1 / delta\n    self.zero_point.data = zero_point\n\n  xr = round_ste(x * self.scale) + self.zero_point\n  xq = torch.clamp(xr, min=self.qlb, max=self.qub)\n  xdq = xq.sub(self.zero_point)\n\n  # dequantize\n  if self.dequantize:\n      xdq = xdq.div(self.scale)\n\n  return xdq\n</code></pre> <p>Different from the original implementation, Torch2Chip enables the SmoothQuant for vision transformers (ViT). </p>"},{"location":"compression/#smoothquantchannelwiseweightquantizer-source","title":"<code>SmoothQuantChannelWiseWeightQuantizer</code> [Source]","text":"<pre><code>class SmoothQuantChannelWiseWeightQuantizer(SmoothQuantizer):\n    def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = False, num_channels:int = 1):\n        self.num_channels = num_channels\n        super().__init__(nbit, train_flag, unsigned)\n\n        # smoother\n        self.smoother = MulShift()\n\n        # observer\n        self.observer = MinMaxChannelWiseWeightObserver(nbit=self.nbit, unsigned=unsigned)\n        self.register_qparams()\n\n    def register_qparams(self):\n        self.register_buffer(\"scale\", torch.ones(self.num_channels, 1))\n        self.register_buffer(\"zero_point\", torch.zeros(self.num_channels, 1))\n</code></pre> <p>Channel-wise quantizer for weight tensors. Inherited from the tensor-wise <code>SmoothQuantizer</code>, together with the channel-wise min-max observer (<code>MinMaxChannelWiseWeightObserver</code>). </p>"},{"location":"compression/#smoothquanttokenwisequantizer-source","title":"<code>SmoothQuantTokenWiseQuantizer</code> [Source]","text":"<pre><code>class SmoothQuantTokenWiseQuantizer(SmoothQuantizer):\n    def __init__(self, nbit: int, train_flag: bool = True, unsigned: bool = True, num_tokens: int = 197):\n        self.num_tokens = num_tokens\n        super().__init__(nbit, train_flag, unsigned)\n\n        # smoother\n        self.smoother = MulShift()\n\n        # observer\n        self.observer = MinMaxTokenWiseObserver(nbit=self.nbit, unsigned=unsigned)\n        self.register_qparams()\n\n    def sync_tokens(self):\n        if self.num_tokens != self.observer.num_tokens:\n            self.observer.num_tokens = self.num_tokens\n\n    def update_qparam(self, input:torch.Tensor):\n        if len(input.shape) == 4:\n            if input.shape[2] != self.num_tokens:\n                self.num_tokens = input.shape[2]\n                self.register_qparams()\n                self.observer.register_range()\n\n        elif len(input.shape) == 3:\n            if input.shape[1] != self.num_tokens:\n                self.num_tokens = input.shape[1]\n                self.register_qparams()\n                self.observer.register_range()\n\n        self.sync_tokens()\n\n    def register_qparams(self):\n        self.register_buffer(\"scale\", torch.ones(1, self.num_tokens, 1))\n        self.register_buffer(\"zero_point\", torch.zeros(1, self.num_tokens, 1))\n\n    def trainFunc(self, input: torch.Tensor):\n        self.update_qparam(input)\n\n        return super().trainFunc(input)\n</code></pre> <p>Token-wise quantizer for activation quantization, inherited from the base <code>SmoothQuantizer</code>. </p> <p>Attributes:</p> <ul> <li><code>update_qparam</code>: Update the shape of the quantization parameters with the given input tensor. </li> <li><code>sync_tokens</code>: Synchronize the token size (<code>num_tokens</code>) between the quantizer and the token-wise observer. </li> </ul>"},{"location":"compression/#pruning","title":"Pruning","text":"<p>Pruning sparsifies the weight tensor by removing the weight element with certain importance metrics (e.g., magnitude score). In Torch2Chip, we provide both element-wise pruning and N:M structured fine-grained sparsity. </p>"},{"location":"compression/#elementprune-source","title":"<code>ElementPrune</code> [Source]","text":"<pre><code>class ElementPrune(Pruner):\n    def __init__(self, \n                model: Module, \n                prune_ratio: float, \n                warmup: int, \n                final_epoch: int, \n                dataloader, \n                prune_freq: float, \n                prune_decay = None, \n                regrow: bool = True,\n                init_sparsity: float = 0.0\n            ):\n        super().__init__(model, prune_ratio, warmup, final_epoch, dataloader, prune_freq, prune_decay, regrow)\n        self.init_sparsity = init_sparsity\n        self.init_density = 1 - self.init_sparsity\n\n        self.init_schedule()\n\n        if self.init_density &lt; 1.0:\n            self.erk(self.init_density)\n</code></pre> <p>Element-wise pruner based on magnitude pruning and gradient-based regrow. The prune-and-regrow mechanism is adopted from Sparse Training via Boosting Pruning Plasticity with Neuroregeneration (Liu, NeurIPS, 2021).</p> <p>Attributes:</p> <ul> <li><code>model</code>: DNN model constructed by <code>_QBaseConv2d</code> or <code>_QBaseLinear</code> layers. </li> <li><code>prune_ratio</code>: Target weight sparsity. </li> <li><code>warmup</code>: Warmup epoch before starting the pruning. </li> <li><code>final_epoch</code>: Final epoch of updating sparsity. </li> <li><code>data_loader</code>: Training dataset loader</li> <li><code>prune_freq</code>: Interval between the consecutive sparsity update steps. </li> <li><code>prune_decay</code>: Gradually reduce the ratio of prune-and regrow (Liu, NeurIPS, 2021).</li> <li><code>regrow</code>: Enable the prune-and-regrow or not, <code>regrow=False</code> means perform vanilla magnitude-based pruning. </li> <li><code>init_sparsity</code>: Initial sparsity initialized via ERK. </li> </ul>"},{"location":"compression/#nmpruner-source","title":"<code>NMPruner</code> [Source]","text":"<pre><code>class NMPruner(Pruner):\n    def __init__(self, \n                model: Module, \n                prune_ratio: float, \n                warmup: int, \n                final_epoch: int, \n                dataloader, \n                prune_freq: float, \n                prune_decay=None, \n                regrow: bool = True,\n                M: int = 4,\n                N: int = 2,\n                init_sparsity: float = 0.0\n            ):\n        super().__init__(model, prune_ratio, warmup, final_epoch, dataloader, prune_freq, prune_decay, regrow)\n        # group size\n        self.M = M\n        self.N = N\n\n        # pruning probability\n        self.init_sparsity = init_sparsity\n        self.init_density = 1 - self.init_sparsity\n        self.init_schedule()\n</code></pre> <p>Exploiting structured fine-grained N:M sparsity with group-wise prune and regrow. Gradually increase the percentage of the N:M groups across the entire model. </p> <p>Attributes:</p> <ul> <li><code>model</code>: DNN model constructed by <code>_QBaseConv2d</code> or <code>_QBaseLinear</code> layers. </li> <li><code>prune_ratio</code>: Target weight sparsity. </li> <li><code>warmup</code>: Warmup epoch before starting the pruning. </li> <li><code>final_epoch</code>: Final epoch of updating sparsity. </li> <li><code>data_loader</code>: Training dataset loader</li> <li><code>prune_freq</code>: Interval between the consecutive sparsity update steps. </li> <li><code>prune_decay</code>: Gradually reduce the ratio of prune-and regrow (Liu, NeurIPS, 2021).</li> <li><code>regrow</code>: Enable the prune-and-regrow or not, <code>regrow=False</code> means perform vanilla magnitude-based pruning. </li> <li><code>init_sparsity</code>: Initial sparsity initialized via ERK. </li> <li><code>N</code>: Number of dense elements within the group size <code>M</code>. </li> <li><code>M</code>: Number of weight elements of each group. </li> </ul>"},{"location":"export/","title":"T2C Core Modules","text":""},{"location":"export/#vanilla4compress-source","title":"<code>Vanilla4Compress</code> [Source]","text":"<p>The pre-trained model from PyTorch, timm, and HuggingFace are mainly constructed by the vanilla <code>torch.nn</code> layers. To enable the quantization with Torch2Chip, quantizers and pruning masks should be properly assigned. As a result, converting the vanilla layers into T2C-compatible layers is critical for successful compression. </p> <p>Convert the vanilla PyTorch layer (E.g., <code>torch.nn.Conv2d</code>) into T2C layers (E.g., <code>QBaseConv2d</code>). Layer conversion is the starting point of all the subsequent compression (pruning and quantization).</p> <pre><code>class Vanilla4Compress(object):\n    def __init__(self, model:nn.Module, wbit:int=8, abit:int=8, state_dict:Dict=None) -&gt; None:\n        self.model = model\n        self.wbit = wbit\n        self.abit = abit\n        self.state_dict = state_dict\n</code></pre> <p>Attributes:</p> <ul> <li><code>model</code>: Vanilla pre-trained model <code>torch.nn.Module</code>.</li> <li><code>wbit</code>: Weight precision (<code>int</code>)</li> <li><code>abit</code>: Activation precision (<code>int</code>)</li> <li><code>state_dict</code>: State dict of the pre-trained model (e.g., quantized, pre-trained), the pre-trained model state dict serves as the reference model dictionary to reshape the quantization parameters. </li> </ul> <p>Methods:</p> <ul> <li><code>convert(self)</code>: Convert the entire vanilla model by Iterating over the model and convert the vanilla modules into T2C modules. </li> <li><code>conv(self, layer:torch.nn.Conv2d)</code>: Convert the vanilla <code>Conv2d</code> layer to the T2C compatibile <code>_QBaseConv2d</code>. Copy the pre-trained full-precision weights and bias to the newly-initiated layer. </li> <li><code>linear(self, layer:torch.nn.Linear)</code>: Convert the vanilla <code>Linear</code> layer to the T2C compatibile <code>_QBaseLinear</code>. Copy the pre-trained full-precision weights and bias to the newly-initiated layer. </li> <li><code>mlp(self, layer:Mlp)</code>: Convert all the <code>Linear</code> layer of the vanilla <code>timm.layers.Mlp</code> into the T2C compatibile <code>_QBaseLinear</code>. </li> <li> <p><code>attn(self, layer:Attention)</code>: Convert the vanilla <code>timm</code> Attention module (<code>Attention</code>) into the T2C compatible <code>QAttention</code> module, which further enable the low precision Q, K, V, and attention via quantization. </p> </li> <li> <p><code>wattn(self, layer:WindowAttention)</code>: Convert the vanilla <code>timm</code> Window Attention module (<code>WindowAttention</code>) into the T2C compatible <code>QWindowAttention</code> module, which further enable the low precision Q, K, V, and attention via quantization. </p> </li> <li><code>assign_quantizer(self, wqtype, xqtype)</code>: Assign quantizer to the converted layers based on the type of weight quantizer  (<code>wqtype</code>)  and activation quantizer (<code>xqtype</code>).</li> <li><code>reshape_quantizer</code>: Reshape the quantization parameters with the given quantizers (weight and activation). Since </li> </ul> <p>Example Usage: Convert the pre-trained model and prepare for quantization / pruning </p> <pre><code>from torchvision.models import resnet50, ResNet50_Weights\n\n# Pre-trained ResNet-50 model from PyTorch\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# Vanilla to T2C-compatible layer\nconverter = Vanilla4Compress(model, wbit=args.wbit, abit=args.abit)\nmodel = converter.convert()\n</code></pre> <p>Vanilla ResNet Bottleneck block before conversion:</p> <pre><code>(0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      ...\n)\n</code></pre> <p>Converted ResNet Bottleneck block after conversion:</p> <pre><code>(0): Bottleneck(\n      (conv1): _QBaseConv2d(\n        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (wq): _QBase(\n          nbit=8\n          (observer): BaseObserver()\n        )\n        (aq): _QBase(\n          nbit=8\n          (observer): BaseObserver()\n        )\n        (yq): _QBase(\n          nbit=8\n          (observer): BaseObserver()\n        )\n      )\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        ...\n)\n</code></pre> <p>By default, the <code>_QBase</code> module act like a placeholder module with no quantization methods activated. The user-selected quantizaters will be assigned in the <code>PTQ</code> Trainer. For more details about the post-training quantization module, please refer to the Tutorial section. </p> <p>Example Usage: Reload the fake-quantized model</p> <pre><code># Define a pre-trained model (full-precision)\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nwrapper = Vanilla4Compress\n\n# Load a fully quantized model\nlogger.info(\"=&gt; loading checkpoint...\")\nstate_tmp = load_checkpoint(ckpt=args.resume, state=model.state_dict())\n\n# reload_fake_quant: convert the model and assign quantizers\nconverter = wrapper(model, wbit=args.wbit, abit=args.abit, state_dict=state_tmp)\nmodel = converter.reload_fake_quant(wqtype=args.wqtype, xqtype=args.xqtype)\n\n# load the pre-trained checkpoint (fake-quantized model)\nmodel.load_state_dict(state_tmp)\n</code></pre> <p>Vanilla ResNet Bottleneck block before conversion:</p> <pre><code>(0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      ...\n)\n</code></pre> <p>After converting the vanilla layer and assign the weight and activation quantizers (fake-quantization version) with <code>QDrop</code> and <code>MinMaxChannelWiseWeightQuantizer</code>:</p> <pre><code>(0): Bottleneck(\n      (conv1): _QBaseConv2d(\n        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (wq): MinMaxChannelWiseWeightQuantizer(\n          nbit=8\n          (observer): MinMaxChannelWiseWeightObserver()\n        )\n        (aq): QDrop(\n          nbit=8, delta=1.00e+00\n          (observer): LSQObserver()\n        )\n        (yq): _QBase(\n          nbit=8\n          (observer): BaseObserver()\n        )\n      )\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      ...\n)\n</code></pre> <p>The conversion between the vanilla layer (<code>torch.nn</code>) and the T2C layer is required for conversion and post-compression conversion. </p>"},{"location":"export/#vitv4c-source","title":"<code>ViTV4C</code> [Source]","text":"<p>Vanilla to T2C model conversion designed for vision transformer (ViT) models, compatible for both swin transformer and conventional ViT model architecture, inherited from <code>Vanilla4Compress</code>. </p> <pre><code>class ViTV4C(Vanilla4Compress):\n    def __init__(self, model: nn.Module, wbit: int = 8, abit: int = 8, state_dict:Dict = None) -&gt; None:\n        super().__init__(model, wbit, abit, state_dict)\n</code></pre> <p>Attributes:</p> <ul> <li><code>model</code>: Vanilla pre-trained vision transformer model (<code>torch.nn.Module</code>).</li> <li><code>wbit</code>: Weight precision (<code>int</code>)</li> <li><code>abit</code>: Activation precision (<code>int</code>)</li> <li><code>state_dict</code>: State dict of the pre-trained model (e.g., quantized, pre-trained), the pre-trained model state dict serves as the reference model dictionary to reshape the quantization parameters. </li> </ul> <p>Methods:</p> <ul> <li><code>reshape_quantizer</code>: Reshape the quantization parameters based on the reference parameter dictionary (<code>self.state_dict</code>).</li> <li><code>assign_quantizer</code>: Assign quantizers to Attention and MLP modules of the transformer model.</li> </ul>"},{"location":"export/#bert4compress-source","title":"<code>BERT4Compress</code> [Source]","text":"<p>Vanilla to T2C model conversion for BERT model, inherited from <code>Vanilla4Compress</code>. </p> <pre><code>class BERT4Compress(Vanilla4Compress):\n    def __init__(self, model: nn.Module, wbit: int = 8, abit: int = 8, state_dict: Dict = None) -&gt; None:\n        super().__init__(model, wbit, abit, state_dict)\n\n        self.config = self.model.config\n</code></pre> <p>Attributes:</p> <ul> <li><code>model</code>: Vanilla pre-trained BERT model (<code>torch.nn.Module</code>), must be a HuggingFace pre-trained checkpoint. </li> <li><code>wbit</code>: Weight precision (<code>int</code>)</li> <li><code>abit</code>: Activation precision (<code>int</code>)</li> <li><code>state_dict</code>: State dict of the pre-trained model (e.g., quantized, pre-trained), the pre-trained model state dict serves as the reference model dictionary to reshape the quantization parameters. </li> </ul> <p>Methods:</p> <ul> <li><code>bert_attn(self, layer:BertSelfAttention)</code>: Convert the vanilla BERT Self-Attention module into T2C \"compressable\" attention module. </li> <li><code>bert_output(self, layer:BertSelfOutput)</code>: Convert the fully connected layer of the vanilla BERT output MLP block into T2C \"compressable\" linear layer <code>_QBaseLinear</code>. </li> </ul>"},{"location":"export/#t2c-source","title":"<code>T2C</code> [Source]","text":"<p>Applies post-compression (e.g., Post-training quantization) conversion with a given fake-compressed model. </p> <pre><code>class T2C(object):\n    def __init__(self, model:nn.Module, swl:int, sfl:int, args):\n        self.swl = swl\n        self.sfl = sfl\n        self.args = args\n\n        self.swl = swl\n        self.sfl = sfl\n        self.args = args\n\n        # model fusion\n        fuser = FUSERS[str(args.model)](model)\n\n        # switch to inference mode\n        fuser.inference()\n\n        # fuse layers\n        fused_model = fuser.fuse()\n        self.model = fused_model\n</code></pre> <p><code>T2C</code> module converts a pre-trained fake-quantized model into a hardware-deployable model with low precision only operation. </p> <p>Attributes:</p> <ul> <li><code>model</code>: Pre-trained fake-quantized model. </li> <li><code>swl</code>: Total bitwidth of the fused scaling factor. </li> <li><code>sfl</code>: Fractional bitwidth of the fused scaling factor. </li> <li><code>args</code>: Argument holder (Argparse)</li> <li><code>sparsity</code>: Weight sparsity. </li> </ul> <p>Methods:</p> <ul> <li><code>compute_sparsity</code>:  Compute the weight sparsity of the given model <code>self.model</code>.</li> <li><code>scale_bias2int</code>:  Convert the floating point scaling factor (fused quantization + normalization) and bias into fixed point integer, with the total bitwidth <code>self.swl</code> and fractional bitwidth <code>self.sfl</code>. </li> <li><code>fused_model</code>: Return the converted model with low precision-only operations and fixed-point integer scaling factor and bias. </li> <li><code>export</code>: Export the converted model by fetching and saving the intermediate operator (<code>ops</code>) results (input and output). </li> <li><code>bert_export</code>: Export the converted BERT model by fetching and saving the intermediate operator (<code>ops</code>) results (input and output).</li> </ul>"},{"location":"export/#example-usage","title":"Example Usage","text":"<pre><code># Step 1. Define the model architecture\nmodel = vit_small_patch16_224(pretrained=True)\nwrapper = ViTV4C\n\n# Step 2. Reconfigure the fake-quantization quantizers\nconverter = wrapper(model, wbit=args.wbit, abit=args.abit, state_dict=state_tmp)\nmodel = converter.reload_fake_quant(wqtype=args.wqtype, xqtype=args.xqtype)\n\n# Step 3. Load the pre-trained checkpoint\nstate_tmp = load_checkpoint(ckpt=args.resume, state=model.state_dict())\nmodel.load_state_dict(state_tmp)\n</code></pre> <p>After defining the model architecture and reload the quantizers, the quantizers (in the fake-quantization mode) are properly inserted into the model:</p> <pre><code>(attn): QAttention(\n    (qkv): _QBaseLinear(\n      in_features=384, out_features=1152, bias=True\n      (wq): AdaRound(\n        nbit=8\n        (observer): AdaRoundObserver()\n      )\n      (aq): _QBase(\n        nbit=8\n        (observer): BaseObserver()\n      )\n    (q_norm): Identity()\n    (k_norm): Identity()\n    (attn_drop): Dropout(p=0.0, inplace=False)\n    (proj): _QBaseLinear(\n      in_features=384, out_features=384, bias=True\n      (wq): AdaRound(\n        nbit=8\n        (observer): AdaRoundObserver()\n      )\n      (aq): _QBase(\n        nbit=8\n        (observer): BaseObserver()\n      )\n    )\n    (proj_drop): Dropout(p=0.0, inplace=False)\n    (qkv_scale): MulShift()\n    (qkv_deq): MulShift()\n    (attn_scale): MulShift()\n    (xq): LSQTokenWise(\n      nbit=8, delta_mean=6.12e-02\n      (observer): LSQTokenWiseObserver()\n    )\n    (qqkv): LSQTokenWise(\n      nbit=8, delta_mean=8.10e-02\n      (observer): LSQTokenWiseObserver()\n    )\n    (qproj): LSQTokenWise(\n      nbit=8, delta_mean=3.87e-02\n      (observer): LSQTokenWiseObserver()\n    )\n  )\n</code></pre> <p>By converting the model via <code>T2C</code>, the computation of the forward pass will be executed through the evaluation branch (<code>evalFunc</code>) with low-precision only operations (<code>ops</code>). The quantization and dequantization scaling are fused into the <code>MulShift</code> and <code>MulQuant</code> modules (see below for more details). </p> <pre><code># Step 4. T2C\nt2c = T2C(model=model, swl=args.swl, sfl=args.sfl, args=args)\nqmodel = t2c.fused_model()\n</code></pre> <p>After the T2C conversion:</p> <pre><code>(attn): QAttention(\n    (qkv): _QBaseLinear(\n      in_features=384, out_features=1152, bias=True\n      (wq): AdaRound(\n        nbit=8\n        (observer): AdaRoundObserver()\n      )\n      (aq): _QBase(\n        nbit=8\n        (observer): BaseObserver()\n      )\n      (ops): IntMatMul()\n    )\n    (q_norm): Identity()\n    (k_norm): Identity()\n    (attn_drop): Dropout(p=0.0, inplace=False)\n    (proj): _QBaseLinear(\n      in_features=384, out_features=384, bias=True\n      (wq): AdaRound(\n        nbit=8\n        (observer): AdaRoundObserver()\n      )\n      (aq): _QBase(\n        nbit=8\n        (observer): BaseObserver()\n      )\n      (ops): IntMatMul()\n    )\n    (proj_drop): Dropout(p=0.0, inplace=False)\n    (qkv_scale): MulShift()\n    (qkv_deq): MulShift()\n    (attn_scale): MulShift()\n    (xq): LSQTokenWise(\n      nbit=8, delta_mean=4.63e-02\n      (observer): LSQTokenWiseObserver()\n    )\n    (qqkv): MulQuant()\n    (qproj): MulQuant()\n    (qk): IntMatMul()\n    (attnv): IntMatMul()\n  )\n</code></pre> <p>The model conversion and fusion process of T2C aims to isolate the bottom-level basic operators (e.g., MatMul) on the top level, so the observability of the operation can be ensured for hardware design and verification. </p> <p>As shown in the example of <code>QAttention</code> above, all the matrix multiplication operations (Fully-connected layer, Query x Key, etc) has been isolated into the matrix multiplication operator <code>IntMatMul</code>. </p> <p>In the meantime, the quantizers (<code>qqkv</code>, <code>qproj</code>)  are converted into the <code>MulQuant</code> module, which collectively performs dequantization + quantization. In hardware implementation, the <code>MulQuant</code> operator corresponds to the scaling + rounding operator. </p>"},{"location":"export/#fuser","title":"<code>Fuser</code>","text":"<p>Layer fusers are critical to convert the fake-quantized computation into the actual quantized operation. More importantly, the basic operator of each type of layer(e.g., <code>MatMul</code> in  <code>torch.nn.Linear</code>) is isolated on the top level. As a result, the <code>T2C</code> module can fetch and extract the intermediate input and output for hardware verification with actual low-precision tensors. </p>"},{"location":"export/#layerfuser-source","title":"<code>LayerFuser</code> [Source]","text":"<pre><code>class LayerFuser(object):\n    def __init__(self, model:nn.Module):\n</code></pre> <p>Fuse the layers between <code>conv</code>, <code>quantizer</code>, and <code>batchnorm</code>, the base class for all the other model-specific fusers. </p> <p>Attributes / Methods:</p> <ul> <li><code>model</code>: DNN model with T2C layers.</li> <li><code>inference(self)</code> : Switch the model to the inference mode. </li> </ul>"},{"location":"export/#fusion-between-conv-bn-and-relu","title":"Fusion between Conv, BN, and ReLU","text":"<p>Given a sequence of Conv, BatchNorm, and ReLU layers, the dequantization and the normalization layer can be fused as a channel-wise scaling and bias adding process. Specifically, given the quantized weight \\(W_Q\\) and activation \\(X_Q\\), the final output can be characterized as: $$ Y = Round(\\frac{\\gamma}{\\sigma}\\frac{S_XS_W}{S_Y}(X_Q * W_Q) + \\frac{1}{S_Y}(\\beta - \\frac{\\gamma \\mu}{\\sigma})) $$ Where \\(\\gamma\\),  \\(\\sigma\\) , \\(\\beta\\), and \\(\\mu\\) represents the weight, running variance, bias, and running mean of the BatchNorm module. </p> <p>It is easy to tell that the low precision operation (\\(X_Q *W_Q\\)) is executed in the dedicated computation engine, while the post-computation scaling and bias adding are deployed to the high-precision operator. Conceptually, the overall hardware architecture can be generalized as:</p> <p></p> <p>Where the fused scaling factor and fused bias tensors can be quantized into fixed-point representation (<code>scale_bias2int</code>) with high precision representation (e.g., INT16). </p>"},{"location":"export/#qconvbnrelu-source","title":"<code>QConvBNReLU</code> [Source]","text":"<pre><code>class QConvBNReLU(nn.Module):\n    r\"\"\"\n    Template of module fusion\n    \"\"\"\n    def __init__(self, in_channels:int, out_channels:int, kernel_size:int, stride:int=1, \n                padding:int=0, dilation:int=1, groups:int=1, bias:bool=True, wbit:int=32, \n                abit:int=32, train_flag=True, int_out=True):\n        super(QConvBNReLU, self).__init__()\n\n        # modules\n        self.conv = _QBaseConv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, wbit, abit, train_flag)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.Identity()\n\n        # flag\n        self.int_out = int_out\n        self.scale = torch.tensor(1.0)\n\n        # precision\n        self.abit = abit\n\n        # scaler\n        self.scaler = MulShift()\n</code></pre> <p>For Convolutional Neural Network, the sequence of <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers are collectively \"fused\" into the <code>QConvBNReLU</code> layer.</p> <p>The combined scaling factor is fused into the <code>scaler</code> , while the convolution is perfomed with the quantized low precision tensors (weight and activation). </p> <pre><code>def forward(self, inputs:torch.Tensor) -&gt; torch.Tensor:\n    x = self.conv(inputs)\n    x = self.scaler(x)\n    x = self.relu(x)\n    return x\n</code></pre>"},{"location":"export/#mulshift-source","title":"<code>MulShift</code> [Source]","text":"<pre><code>class MulShift(nn.Module):\n    r\"\"\"Multiply the scaling factor and add the bias\n\n    Attributes:\n    scale: Scaling factor with the shape of output channels.\n    bias: Bias value. \n    fl: Fractional bits of the high-precision integer.\n    \"\"\"\n    def __init__(self):\n        super(MulShift, self).__init__()\n        self.register_buffer(\"scale\", torch.tensor(1.0))\n        self.register_buffer(\"bias\", torch.tensor(0.0))\n\n        # fractional bit width\n        self.fl = 0.\n\n    def inference(self):\n        pass\n\n    def forward(self, x:torch.Tensor):\n        out = x.mul(self.scale).add(self.bias)\n        out = out.mul(2**(-self.fl))\n        return out\n</code></pre> <p>Multiply the scaling factor and adding bias of the input tensor from the previous computation step (e.g., Convolution or MatMul). </p> <p>In particular, if the scaling factor and bias are quantized to the fixed point high-precision integer format, the scaled tensor will be right shifted by the number of fractional bits. </p>"},{"location":"export/#mulquant-source","title":"<code>MulQuant</code> [Source]","text":"<pre><code>class MulQuant(nn.Module):\n    r\"\"\"Multiply the scaling factor and add the bias, then quantize the output.\n\n    Attributes:\n    scale: Scaling factor with the shape of output channels.\n    bias: Bias value. \n    fl: Fractional bits of the high-precision integer.\n    \"\"\"\n    def __init__(self, nbit:int=8, unsigned=False):\n        super(MulQuant, self).__init__()\n        self.register_buffer(\"scale\", torch.tensor(1.0))\n        self.register_buffer(\"bias\", torch.tensor(0.0))\n        self.register_buffer(\"zero_point\", torch.tensor(0.0))\n\n        self.nbit = nbit\n        self.unsigned = unsigned\n\n        # upper and lower bound\n        if not self.unsigned:\n            self.qlb = -2**(self.nbit-1)\n            self.qub = 2**(self.nbit-1) - 1\n        else:\n            self.qlb = 0\n            self.qub = 2**(self.nbit) - 1\n\n        # fractional bit width\n        self.fl = 0.\n\n    def inference(self):\n        pass\n\n    def forward(self, x:torch.Tensor):\n        # scale\n        out = x.mul(self.scale)\n        out = out.add(self.bias).mul(2**(-self.fl)).round()\n\n        # quant\n        out = out.add(self.zero_point)\n        out = out.clamp(min=self.qlb, max=self.qub).sub(self.zero_point)\n\n        return out.clamp(min=self.qlb, max=self.qub)\n</code></pre> <p>Multiply the scaling factor and adding bias of the input tensor from the previous computation step (e.g., Convolution or MatMul), then round the scaled tensor to formulate the quantized low precision output.</p> <p>In particular, if the scaling factor and bias are quantized to the fixed point high-precision integer format, the scaled tensor will be right shifted by the number of fractional bits. </p>"},{"location":"export/#convolutional-neural-network","title":"Convolutional Neural Network","text":""},{"location":"export/#resnet18fuser-source","title":"<code>ResNet18Fuser</code> [Source]","text":"<pre><code>class ResNet18Fuser(LayerFuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Apply layer-wise fusion for the ResNet-18 model. Iterating over the ResNet <code>BasicBlock</code> and fuse the <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers. The resultant model performs the convolution of each layer with quantized tensors only (weight and activation) through the inference path (<code>evalFunc</code>).</p>"},{"location":"export/#resnet34fuser-source","title":"<code>ResNet34Fuser</code> [Source]","text":"<pre><code>class ResNet34Fuser(ResNet18Fuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Apply layer-wise fusion for the ResNet-34 model. Iterating over the ResNet <code>BasicBlock</code> and fuse the <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers. The resultant model performs the convolution of each layer with quantized tensors only (weight and activation) through the inference path (<code>evalFunc</code>).</p> <p>Note: Since the ResNet-34 and ResNet-18 shares the same basic module (<code>BasicBlock</code>), <code>ResNet34Fuser</code> and <code>ResNet18Fuser</code> are sharing the same fusing strategy. </p>"},{"location":"export/#resnet50fuser-source","title":"<code>ResNet50Fuser</code> [Source]","text":"<pre><code>class ResNet50Fuser(LayerFuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Apply layer-wise fusion for the ResNet-50 model. Iterating over the ResNet <code>BottleneckBlock</code> and fuse the <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers. The resultant model performs the convolution of each layer with quantized tensors only (weight and activation) through the inference path (<code>evalFunc</code>).</p>"},{"location":"export/#mobilenetv1fuser-source","title":"<code>MobileNetV1Fuser</code> [Source]","text":"<pre><code>class MobileNetV1Fuser(LayerFuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Apply layer-wise fusion for the MobileNet-V1 model. Iterating over the model and fuse the  <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers. For the convolutional layers, both depth-wise and point-wise layers are fused properly. The resultant model performs the convolution of each layer with quantized tensors only (weight and activation) through the inference path (<code>evalFunc</code>)</p>"},{"location":"export/#vggfuser-source","title":"<code>VGGFuser</code> [Source]","text":"<pre><code>class VGGFuser(LayerFuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Apply layer-wise fusion for the VGG model. Iterating over the model and fuse the  <code>_QBaseConv2d</code>, <code>BatchNorm2d</code>, and <code>ReLU</code> layers. The resultant model performs the convolution of each layer with quantized tensors only (weight and activation) through the inference path (<code>evalFunc</code>). </p>"},{"location":"export/#transformer","title":"Transformer","text":""},{"location":"export/#vitfuser-source","title":"<code>ViTFuser</code> [Source]","text":"<pre><code>class ViTFuser(object):\n    def __init__(self, model: nn.Module):\n        self.model = model.eval()\n</code></pre> <p>Iterate over the <code>VisionTransformer</code> model and fuse the <code>Attention</code> and <code>MLP</code> modules and enable the integer-only operations. </p> <p>Different from the fusion for convolutional neural networks, the post-training fusion of the transformer model ensures the low precision operations inside both MLP and Attention modules:</p> <p>Attention Module:</p> <ul> <li>Low precision operation of the QKV projection <code>qkv</code> (low precision weight + activation).</li> <li>Low precision MatMul between Q and K. </li> <li>Low precision output of SoftMax function. </li> <li>Low precision MatMul between Attention and V. </li> <li>Low precision Projection layer. </li> </ul> <p>MLP:</p> <ul> <li>Low precision MatMul operation of fully-connected layers (<code>fc1</code> and <code>fc2</code>).</li> </ul> <p>The forward pass will be executed through the evaluation path (<code>evalFunc</code>) of the <code>QAttention</code> module with integer-only MatMul operations. </p>"},{"location":"export/#swinfuser-source","title":"<code>SwinFuser</code> [Source]","text":"<pre><code>class SwinFuser(ViTFuser):\n    def __init__(self, model: nn.Module):\n        super().__init__(model)\n</code></pre> <p>Model fuser for SwinTransformer. Fuse the SwinAttention and MLP modules and enable the integer-only computations. Similar to the <code>ViTFuser</code>, the forward pass fo the converted SwinTransformer model follows the evaluation path (<code>evalFunc</code>) with the following integer-only operations:</p> <p>Attention Module:</p> <ul> <li>Low precision operation of the QKV projection <code>qkv</code> (low precision weight + activation).</li> <li>Low precision MatMul between Q and K. </li> <li>Low precision output of SoftMax function. </li> <li>Low precision MatMul between Attention and V. </li> <li>Low precision Projection layer. </li> </ul> <p>MLP:</p> <ul> <li>Low precision MatMul operation of fully-connected layers (<code>fc1</code> and <code>fc2</code>).</li> </ul>"},{"location":"pretrained/","title":"Pre-trained Model","text":""},{"location":"pretrained/#convolutional-neural-networks-imagenet","title":"Convolutional Neural Networks (ImageNet)","text":""},{"location":"pretrained/#resnet-18","title":"ResNet-18","text":"<p>[Uploading to the university cloud drive. Coming Soon]</p>"},{"location":"pretrained/#resnet-34","title":"ResNet-34","text":"<p>[Uploading to the university cloud drive. Coming Soon]</p>"},{"location":"pretrained/#resnet-50","title":"ResNet-50","text":"<p>\u200b   [A8W8] QDrop + AdaRound [Checkpoint] [Accuracy = 76.00]</p> <p>\u200b   [A8W8] QDrop + MinMaxChannel [Checkpoint] [Accuracy = 76.21]</p> <p>\u200b   [A8W8] LSQ + MinMaxChannel [Checkpoint] [Accuracy = 76.26]  </p> <p>\u200b   [A8W8] LSQ + AdaRound [Checkpoint] [Accuracy = 75.98]</p>"},{"location":"pretrained/#vision-transformer-imagenet","title":"Vision Transformer (ImageNet)","text":""},{"location":"pretrained/#vit-tiny-patch16-224","title":"ViT-Tiny-Patch16-224","text":"<p>\u200b   [A8W8] LSQ + AdaRound [Checkpoint)] [Accuracy = 81.10] </p> <p>\u200b   [A8W8] LSQ + MinMaxChannel [Checkpoint] [Accuracy = 80.99]</p> <p>\u200b   [A8W8] LSQ-TokenWise + AdaRound [Checkpoint] [Accuracy = 80.34]</p> <p>\u200b   [A8W8] LSQ-TokenWise + MinMaxChannel [Checkpoint] [Accuracy = 80.24]</p> <p>\u200b   [A8W8] MinMaxToken + MinMaxChannel [Checkpoint] [Accuracy = 80.05]</p> <p>\u200b   [A8W8] QDrop + AdaRound [Checkpoint] [Accuracy = 81.07]</p> <p>\u200b   [A8W8] QDrop-TokenWise + AdaRound [Checkpoint] [Accuracy = 80.16]   </p> <p>\u200b   [A8W8] QDrop-TokenWise + MinMaxChannel [Checkpoint] [Accuracy = 80.31]</p> <p>\u200b   [A8W8] SmoothQuant + SmoothQuantChannelWise [Checkpoint] [Accuracy = 76.66]</p> <p>\u200b   [A8W8] SmoothQuant-TokenWise + SmoothQuantChannelWise [Checkpoint] [Accuracy = 79.99]</p>"},{"location":"pretrained/#vit-small-patch16-224","title":"ViT-Small-Patch16-224","text":"<p>\u200b   [Uploading to the university cloud drive. Coming Soon]</p>"},{"location":"pretrained/#vit-base-patch16-224","title":"ViT-Base-Patch16-224","text":"<p>\u200b   [Uploading to the university cloud drive. Coming Soon]</p>"},{"location":"tutorials/","title":"Tutorial","text":""},{"location":"tutorials/#set-up-the-torch2chip-environment","title":"Set up the Torch2Chip environment","text":"<p>To start, create the compatible environment based on the conda environment file <code>t2c.yml</code> :</p> <pre><code>conda env create -f t2c.yml\n</code></pre> <p>Specifically, the basic dependencies of torch2chip are:</p> <pre><code>python 3.9.19\ntorch==2.3.0\ntorchaudio==2.3.0\ntorchvision==0.18.0\nhuggingface-hub==0.23.1\ntimm==1.0.3\nfxpmath==0.4.9\n</code></pre> <p>Activate your newly installed conda environment and start using Torch2Chip (see below)</p> <pre><code># To activate this environment, use\n#\n#     $ conda activate t2c\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre>"},{"location":"tutorials/#compress-your-model","title":"Compress your model","text":"<p>In Torch2Chip, we provide various example running scripts for different vision models, starting from the basic CNN (e.g., ResNet) to vision transformers (e.g., ViT). Given the needs of quickly compressing a pre-trained model, we employ post-training quantization (PTQ) as the major compression scheme for all the pre-trained FP32 model. </p> <p>List of examples are located here (link).</p> <p>Currently, Torch2Chip have provided all different kinds of combinations between quantization methods and pre-trained models. </p> <p>Example: Compressing a ViT-Small model on ImageNet with PTQ: </p> <pre><code>if [ ! -d \"$DIRECTORY\" ]; then\n    mkdir ./save\nfi\n\nexport CUDA_VISIBLE_DEVICES=0\n\nmodel=vit_small\nepochs=1\nbatch_size=100\nlr=1e-4\nloss=cross_entropy\nweight_decay=1e-4\ndataset=\"imagenet\"\nlog_file=\"training.log\"\n\nwbit=8\nabit=8\nxqtype=\"lsq_token\"\nwqtype=\"minmax_channel\"\nnum_samples=500\nttype=qattn\nlayer_train=True\n\nsave_path=\"./save/${dataset}/${model}/${xqtype}_${wqtype}/${model}_w${wbit}_a${abit}_lr${lr}_batch${batch_size}_${loss}loss_all/\"\n\npython3 -W ignore ./imagenet/vit.py \\\n    --save_path ${save_path} \\\n    --model ${model} \\\n    --epochs ${epochs} \\\n    --log_file ${log_file} \\\n    --lr ${lr} \\\n    --weight-decay ${weight_decay} \\\n    --batch_size ${batch_size} \\\n    --loss_type ${loss} \\\n    --dataset ${dataset} \\\n    --mixed_prec True \\\n    --optimizer adam \\\n    --trainer ${ttype} \\\n    --wqtype ${wqtype} \\\n    --xqtype ${xqtype} \\\n    --wbit ${wbit} \\\n    --abit ${abit} \\\n    --num_samples ${num_samples} \\\n    --fine_tune \\\n    --train_dir \"/share/seo/imagenet/train/\" \\\n    --val_dir \"/share/seo/imagenet/val/\" \\\n    --layer_trainer ${layer_train} \\\n</code></pre> <p>Arguments:</p> <p><code>model</code>: Model architecture. </p> <ul> <li> <p><code>vit_tiny</code>: <code>vit_tiny_patch16_224</code> pretrained by timm.</p> </li> <li> <p><code>vit_small</code>: <code>vit_base_patch16_224</code> pretrained by timm.</p> </li> <li> <p><code>vit_base</code>: <code>vit_base_patch16_224</code> pretrained by timm.</p> </li> <li> <p><code>swin_tiny_patch4_window7_224</code>: <code>swin_tiny_patch4_window7_224</code> pretrained by timm.</p> </li> <li> <p><code>swin_base_patch4_window7_224</code>: <code>swin_base_patch4_window7_224</code> pretrained by timm.</p> </li> </ul> <p><code>xqtype</code>: Activation quantizer type</p> <ul> <li><code>lsq</code>: Learned Step Size Quantization (LSQ) , Granularity: Tensor-wise.</li> </ul> <p><code>wqtype</code>: Weight quantizer type</p> <ul> <li><code>minmax_channel</code>: Channel-wise MinMax Quantizer. </li> </ul> <p><code>save_path</code>: Model saving path.</p> <ul> <li>By default, the compressed model will be saved inside the <code>./save/</code> folder under the torch2chip directory. </li> </ul> <p><code>epochs</code>: Number of epochs for PTQ. </p> <ul> <li>For fast compression process, we only run one time for each PTQ. </li> </ul> <p><code>log_file</code>: Output log file that shows accuracy and model architecture. </p> <p><code>lr</code>: Learning rate of PTQ (if there are trainable parameters in your quantizers).</p> <p><code>weight-decay</code>: Weight decay and regularization of PTQ (if there are trainable parameters in your quantizers).</p> <p><code>batch_size</code>: Batch size of PTQ calibration. </p> <p><code>loss_type</code>: Calibration loss of PTQ. </p> <p><code>dataset</code>: Dataset / task for the calibration. </p> <p><code>mixed_prec</code>: Enable mixed precision training. </p> <p><code>optimizer</code>: Optimizer of PTQ (if there are trainable parameters in your quantizers).</p> <p><code>trainer</code>: Calibration engine for post-training quantization (Details of different trainers can be found in the Training section. </p> <ul> <li><code>ptq</code>: Calibrator for CNN.</li> <li><code>qattn</code>: Calibrator for vision transformers (inherited from <code>ptq</code>). </li> <li><code>smooth_quant</code>: Calibrator for SmoothQuant (inheirted from <code>qattn</code>).</li> </ul> <p><code>num_samples</code>: Total number of calibration samples (default = 500).</p> <p><code>fine_tune</code>: Flag for fine-tuning and reloading. </p> <p><code>train_dir</code>: Directory of the training set of your dataset. </p> <p><code>val_dir</code>: Directory of the validation set of your dataset. </p> <p><code>layer_trainer</code>: Flag of controlling whether to optimize the trainable quantizer parameter (e.g., for LSQ) or just one shot calibration (e.g., for MinMax quantizer).</p> <p>By executing the script above, torch2chip will start calibrating the model with compression. Please note that, if your model has been pruned by the pruner (link), torch2chip will collectively comrpess the sparse weights into the user-defined precision: \\(W_Q = Quantize(W\\times mask)\\).</p> <pre><code>bash script/imagenet/vit-ptq-minmax-lsq.sh\n</code></pre> <p>After the calibration is completed, the fake-quantized model checkpoint is saved together with the log file:</p> <pre><code>cd ./save/imagenet/vit_small/lsq_minmax_channel/vit_small_w8_a8_lr1e-4_batch100_cross_entropyloss_all\n</code></pre>"},{"location":"tutorials/#convert-the-fake-quantized-model-to-torch2chip-model","title":"Convert the Fake-Quantized model to Torch2Chip model","text":"<p>Now you have a pre-trained fake-quantized model as a starting point. Now we can call T2C to fuse up the module and save all the intermediate input and output results. </p> <p>In the previous example, we use the learned step size quantization for activation and channel-wise minmax quantization for weights. To fuse the fake-quantized model, we elaborate the example of T2C into <code>t2c.py</code> with the execution script <code>vit-t2c-minmax-lsq.sh</code>:</p> <pre><code>if [ ! -d \"$DIRECTORY\" ]; then\n    mkdir ./save\nfi\n\nexport CUDA_VISIBLE_DEVICES=0\n\nmodel=vit_small\nepochs=200\nbatch_size=100\nlr=0.1\nloss=cross_entropy\nweight_decay=0.0005\ndataset=\"imagenet\"\nlog_file=\"t2c.log\"\nwbit=8\nabit=8\nxqtype=\"lsq_token\"\nwqtype=\"minmax_channel\"\nttype=ptq\n\nsave_path=\"./save/imagenet/${model}/${xqtype}_${wqtype}/${model}_w${wbit}_a${abit}_lr1e-4_batch100_cross_entropyloss_all/t2c/\"\npre_trained=\"./save/imagenet/${model}/${xqtype}_${wqtype}/${model}_w${wbit}_a${abit}_lr1e-4_batch100_cross_entropyloss_all/model_best.pth.tar\"\n\npython3 -W ignore ./imagenet/t2c.py \\\n    --save_path ${save_path} \\\n    --model ${model} \\\n    --batch_size ${batch_size} \\\n    --resume ${pre_trained} \\\n    --log_file ${log_file} \\\n    --fine_tune \\\n    --wqtype ${wqtype} \\\n    --xqtype ${xqtype} \\\n    --wbit ${wbit} \\\n    --abit ${abit} \\\n    --dataset ${dataset} \\\n    --train_dir \"/share/seo/imagenet/train/\" \\\n    --val_dir \"/share/seo/imagenet/val/\" \\\n    --evaluate \\\n    --trainer qattn \\\n    --swl 32 \\\n    --sfl 26 \\\n    --export_samples 1 \\\n</code></pre> <p>Arguments:</p> <p>Similar to the PTQ script, the post-compression fusion and model export follows the similar configuration except a couple of changes and unique argument variables.</p> <ul> <li><code>save_path</code>: By default, the fused t2c model <code>t2c_model.pth.tar</code> and intermediate tensor files are saved in the <code>t2c/</code> folder under the path of PTQ directory. </li> <li><code>swl</code>: Total bitwidth of the fused scaling factor (BatchNorm + Quantization). Default = 32bit.</li> <li><code>sfl</code>: Fractional bitwidth of the fused scaling factor (BatchNorm + Quantization). Default = 26bit.</li> <li><code>export_samples</code>: Batch size of the final export tensors. Default = 1. </li> </ul> <p>By executing the bash file, torch2chip will load the pre-trained fake quantized model and run through the fusion and export process. The converted model and the extracted tensors are saved in the <code>save_path</code> and <code>save_path/tensors/</code>, respectively. </p> <pre><code>bash script/imagenet/vit-t2c-minmax-lsq.sh\n</code></pre>"},{"location":"tutorials/#reload-your-torch2chip-model-back-for-verification","title":"Reload your Torch2Chip model back for verification","text":"<p>Now you have your T2C model (<code>t2c_model.pth.tar</code>) with integer-only operations. In practice, you might want to alter the low precision operations with some hardware-aware non-idealities. </p> <p>To reload the t2c model from the previous example (<code>lsq + minmax_channel</code>), we provide the following example (<code>vit-t2c-reload-minmax-lsq.sh</code>) as a starting point: </p> <pre><code>if [ ! -d \"$DIRECTORY\" ]; then\n    mkdir ./save\nfi\n\nexport CUDA_VISIBLE_DEVICES=0\n\nmodel=vit_small\nbatch_size=100\nloss=cross_entropy\nweight_decay=0.0005\ndataset=\"imagenet\"\nlog_file=\"reload.log\"\nwbit=8\nabit=8\nxqtype=\"lsq_token\"\nwqtype=\"minmax_channel\"\nttype=ptq\n\nsave_path=\"./save/imagenet/vit_small/lsq_token_minmax_channel/vit_small_w8_a8_lr1e-4_batch100_cross_entropyloss_all/t2c/\"\npre_trained=\"./save/imagenet/vit_small/lsq_token_minmax_channel/vit_small_w8_a8_lr1e-4_batch100_cross_entropyloss_all/t2c/t2c_model.pth.tar\"\n\npython3 -W ignore ./imagenet/reload.py \\\n    --save_path ${save_path} \\\n    --model ${model} \\\n    --batch_size ${batch_size} \\\n    --resume ${pre_trained} \\\n    --log_file ${log_file} \\\n    --fine_tune \\\n    --wqtype ${wqtype} \\\n    --xqtype ${xqtype} \\\n    --wbit ${wbit} \\\n    --abit ${abit} \\\n    --dataset ${dataset} \\\n    --train_dir \"/share/seo/imagenet/train/\" \\\n    --val_dir \"/share/seo/imagenet/val/\" \\\n    --evaluate \\\n    --trainer qattn \\\n</code></pre> <p>As shown in the example of <code>imagenet/reload.py</code>, the model is reloaded to evaluate the accuracy based on the integer-only operation across the model. With that, users are allowed to customize the operator (e.g., Non-ideal MatMul) with hardware-oriented characteristics. </p>"}]}